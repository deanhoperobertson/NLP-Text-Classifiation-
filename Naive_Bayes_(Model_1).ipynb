{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes (Model 1)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deanhoperobertson/NLP-Text-Classifiation-/blob/master/Naive_Bayes_(Model_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEIwI30lYqBO",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJRf8n7Qn-mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import urllib\n",
        "from sklearn import model_selection, preprocessing\n",
        "\n",
        "#Vizualization\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "#Feature Engineering\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn import model_selection, preprocessing\n",
        "\n",
        "#keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Model Building\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjPSH9txoBPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import data from my github repo\n",
        "url = \"https://raw.githubusercontent.com/deanhoperobertson/NLP-Text-Classifiation-/master/corpus.txt\"\n",
        "data = urllib.request.urlopen(url).read()\n",
        "data = data.decode('utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp_m5ToIoQvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, texts = [], []\n",
        "for i, line in enumerate(str(data).split(\"\\n\")):\n",
        "    content = line.replace(\"\\\\\",\"\").split()\n",
        "    labels.append(content[0])\n",
        "    texts.append(\" \".join(content[1:]))\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6MnALY6reQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split into train and test set (default is 25%)\n",
        "#set seed for consistency\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.25, random_state=1)\n",
        "\n",
        "# encode target labels into intergers between 0 - no.classes\n",
        "# label 2 = 1\n",
        "# label 1 = 0\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llC5k2HholrN",
        "colab_type": "text"
      },
      "source": [
        "### Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0T2pEb4daWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate a count vectorizer object to tokenize on a word level\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', strip_accents =\"ascii\", stop_words=None)\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "#see the unique words being tokenized \n",
        "count_vect.get_feature_names()[1:10]\n",
        "\n",
        "#create a document term matrix using the \n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqxBQ-tKqDFg",
        "colab_type": "text"
      },
      "source": [
        "### TDIF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTmdDO7WqHLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizing on a word level\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsj7kLpArqKa",
        "colab_type": "text"
      },
      "source": [
        "### N-Gram Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO7cPa5FruQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al4Z7eLUowjr",
        "colab_type": "text"
      },
      "source": [
        "## 1. Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRHTVpPte_Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the model build as a function\n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tUNZkJVqY-W",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ6ZeJW1jYZ6",
        "colab_type": "code",
        "outputId": "ca4fd00f-db76-4c4e-8e8b-0367114a9224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: %s %s\" %(accuracy,\"%\"))\n",
        "\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF:  %s %s\" %(accuracy,\"%\"))\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors:  %s %s\" %(accuracy,\"%\"))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors: 0.8336 %\n",
            "NB, WordLevel TF-IDF:  0.8388 %\n",
            "NB, N-Gram Vectors:  0.8412 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Q_3dNClerz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}